# Dockerfiel for rta_hc

# import image rta_hc_conf created by the second dockerfile
FROM rta_hc_conf:latest

# this line will be executed just after import it will run the script
# under the path specified in the container and start all necessary components
ENTRYPOINT ["/opt/rta_hc/start-docker.sh"]

# all below lines are for test purposes thus commented
# Check python path
#WORKDIR /opt/mount1
#RUN touch diag.log
#RUN python --version > diag.log

# Check java path
#RUN java -version >> diag.log

# Check tomcat settings
#RUN echo 'check tomcat' >> diag.log
#WORKDIR $TOMCAT_HOME
#RUN cat conf/server.xml|grep 9797 >> diag.log
#RUN cat webapps/simpleiothttp2kafka/WEB-INF/web.xml|grep 9092 >> diag.log

# Check spark
# RUN echo 'check spark' >> diag.log
# WORKDIR $SPARK_HOME
# RUN cat conf/spark-env.sh|grep SAPRK_HOME >> diag.log
# RUN cat conf/spark-env.sh|grep SPARK_MASTER_WEBUI_PORT >> diag.log
# RUN cat conf/spark-env.sh|grep SPARK_MASTER_IP >> diag.log
# RUN cat conf/spark-env.sh|grep SPARK_LOCAL_IP >> diag.log
# RUN cat conf/spark-env.sh|grep SPARK_WORKER_MEMORY >> diag.log
# RUN cat conf/spark-env.sh|grep HADOOP_CONF_DIR >> diag.log
# RUN cat conf/spark-env.sh|grep PYSPARK_PYTHON >> diag.log
# RUN cat conf/spark-env.sh|grep PYSPARK_DRIVER_PYTHON >> diag.log

# RUN cat conf/spark-defaults.conf|grep spark.master >> diag.log
# RUN cat conf/spark-defaults.conf|grep spark.driver.memory >> diag.log
# RUN cat conf/spark-defaults.conf|grep spark.executor.memory >> diag.log

# RUN cat $SPARK_HOME/conf/slaves|grep localhost >> diag.log

# Check kafka
# RUN echo 'check kafka' >> diag.log
# RUN ls /opt/mount1 >> diag.log
# WORKDIR $KAFKA_HOME
# RUN cat config/zookeeper.properties|grep dataDir >> diag.log
# RUN cat config/server.properties|grep log.dirs >> diag.log
# RUN cat config/server.properties|grep delete.topic.enable >> diag.log

# Check kafka topics config
# RUN bin/kafka-topics.sh --list --zookeeper 0.0.0.0:2181 >> diag.log

# Check cassandra
# RUN echo 'Check cassandra' >> diag.log
# Run ls /opt/mount1/cassandra >> diag.log
# WORKDIR /etc/cassandra
# RUN cat conf/cassandra.yaml|grep seeds >> diag.log 

# RUN cat default.conf/logback.xml|grep "system.log<" >> diag.log
# RUN cat default.conf/logback.xml|grep "debug.log<" >> diag.log

# RUN cat default.conf/cassandra.yaml |grep start_rpc >> diag.log
# RUN cat default.conf/cassandra.yaml |grep rpc_address >> diag.log
# RUN cat default.conf/cassandra.yaml |grep rpc_port >> diag.log

# Check cassandra 1 keyspaces and 15 tables
# RUN cqlsh -e 'describe keyspaces' >> diag.log
# RUN cqlsh -e 'describe tables' >> diag.log

# Check kairosdb
# RUN echo 'check kairosdb' >> diag.log
# WORKDIR $KAIROSDB_HOME
# RUN cat conf/kairosdb.properties | grep jetty.port >> diag.log
# RUN cat conf/kairosdb.properties | grep H2Module >> diag.log
# RUN cat conf/kairosdb.properties | grep concurrent >> diag.log
# RUN cat conf/kairosdb.properties | grep CassandraModule >> diag.log
# RUN cat conf/kairosdb.properties | grep host_list >> diag.log
# RUN cat conf/kairosdb.properties | grep keyspace >> diag.log

# Check grafana
# RUN echo 'check grafana' >> diag.log
# RUN cat /etc/grafana/grafana.ini | grep plugin.kairosdb >> diag.log
# RUN cat /etc/grafana/grafana.ini | grep kairosdb-datasource-master >> diag.log

# Check kairosdb start
# RUN curl POST http://0.0.0.0:9090/api/v1/datapoints \
#[{"name":"accelerometer-x","timestamp":1458632210606,"value":"0.991211","tags":{"deviceid":"b81e905908354542","userid":"vincent.planat@hpe.com"}},\
#{"name":"accelerometer-y","timestamp":1458632210606,"value":"-0.027588","tags":{"deviceid":"b81e905908354542","userid":"vincent.planat@hpe.com"}},\
#{"name":"accelerometer-z","timestamp":1458632210606,"value":"0.015625","tags":{"deviceid":"b81e905908354542","userid":"vincent.planat@hpe.com"}}]


